# -*- coding: utf-8 -*-
"""CS767_Thomas_Olson_Assignment_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11qwurTztQ1u2ZB0vwG9MhpxAaNv-5Iza
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from keras.callbacks import LearningRateScheduler
from keras.optimizers import gradient_descent_v2
import matplotlib.pyplot as plt
from keras import models
from keras import layers
import plotly.graph_objs as go
from plotly import tools
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot



print(tf.__version__)
print(keras.__version__)

(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()

X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]
y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]

X_mean = X_train.mean(axis=0, keepdims=True)
X_std = X_train.std(axis=0, keepdims=True) + 1e-7
X_train = (X_train - X_mean) / X_std
X_valid = (X_valid - X_mean) / X_std
X_test = (X_test - X_mean) / X_std

X_train = X_train[..., np.newaxis]
X_valid = X_valid[..., np.newaxis]
X_test = X_test[..., np.newaxis]

initial_learning_rate = 0.01
epochs = 20
decay = initial_learning_rate / epochs
def lr_time_based_decay(epoch, lr):
    return lr * 1 / (1 + decay * epoch)

keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

model = keras.models.Sequential()
#Add first Conv2D layer
model.add(keras.layers.Conv2D(filters=64, kernel_size=7, strides=1,padding="SAME", activation="relu",input_shape=(28, 28, 1)))
model.add(keras.layers.BatchNormalization())
#Add second hidden Conv2D layer
model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,padding="SAME", activation="relu"))
model.add(keras.layers.BatchNormalization())
#Add max pooling 2
model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
model.add(keras.layers.BatchNormalization())
#Add dropout
model.add(keras.layers.Dropout(rate=0.25))
#Add flatten layer before dense layer
model.add(keras.layers.Flatten())
#Add hidden dense layer
model.add(keras.layers.Dense(128, activation="relu"))
model.add(keras.layers.BatchNormalization())
#Add dropout
model.add(keras.layers.Dropout(rate=0.5))
#Add output layer
model.add(keras.layers.Dense(10, activation="softmax"))

model.summary()

model.compile(loss="sparse_categorical_crossentropy", optimizer='nadam', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=20,verbose = 1,validation_data=(X_valid, y_valid),callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1)])

score = model.evaluate(X_test, y_test)
print('Test score:', score[0])
print('Test accuracy:', score[1])

def create_trace(x,y,ylabel,color):
        trace = go.Scatter(
            x = x,y = y,
            name=ylabel,
            marker=dict(color=color),
            mode = "markers+lines",
            text=x
        )
        return trace
    
def plot_accuracy_and_loss(model_fit):
    acc = model_fit.history['accuracy']
    val_acc = model_fit.history['val_accuracy']
    loss = model_fit.history['loss']
    val_loss = model_fit.history['val_loss']
    epochs = list(range(1,len(acc)+1))
    
    trace_ta = create_trace(epochs,acc,"Training accuracy", "Green")
    trace_va = create_trace(epochs,val_acc,"Validation accuracy", "Red")
    trace_tl = create_trace(epochs,loss,"Training loss", "Blue")
    trace_vl = create_trace(epochs,val_loss,"Validation loss", "Magenta")
   
    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',
                                                             'Training and validation loss'))
    fig.append_trace(trace_ta,1,1)
    fig.append_trace(trace_va,1,1)
    fig.append_trace(trace_tl,1,2)
    fig.append_trace(trace_vl,1,2)
    fig['layout']['xaxis'].update(title = 'Epoch')
    fig['layout']['xaxis2'].update(title = 'Epoch')
    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])
    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])

    
    iplot(fig, filename='accuracy-loss')

plot_accuracy_and_loss(history)

