# -*- coding: utf-8 -*-
"""COTI_CS767_Project_Olson.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J9tKKPpoO-fds7JVd3p0_htiefm0lPJO

# Project Summary

**Objective:** The goal of the project is to accuratly predict the category of garbage of an image. A dataset containing 1,868 images of garbage is used to train a Multi-layer Perceptron and Convolutional Neural Networks.

**Notebook Contents:**


*   Data Pre-Processing
*   Data Visualization
*   Model Training & Evaluation

**Data:** The dataset used for this project contains the images of garbage from the following categories: glass, paper, cardboard, plastic, metal and trash. ([Data Source](https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification))

## Import Libraries
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator

from sklearn.metrics import confusion_matrix, f1_score
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.applications import Xception
from tensorflow.keras.applications.xception import preprocess_input
from google.colab import drive

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D
from keras.callbacks import LearningRateScheduler
import math
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import GridSearchCV

tf.__version__

keras.__version__

"""## Import Data"""

drive.mount('/content/drive')  
glass_dir = '/content/drive/MyDrive/Garbage-classification/Garbage-classification/glass/'
paper_dir = '/content/drive/MyDrive/Garbage-classification/Garbage-classification/paper/'
cardboard_dir = '/content/drive/MyDrive/Garbage-classification/Garbage-classification/cardboard/'
plastic_dir = '/content/drive/MyDrive/Garbage-classification/Garbage-classification/plastic/'
metal_dir = '/content/drive/MyDrive/Garbage-classification/Garbage-classification/metal/'
trash_dir =  '/content/drive/MyDrive/Garbage-classification/Garbage-classification/trash/'
label_dir = [glass_dir, paper_dir, cardboard_dir, plastic_dir, metal_dir, trash_dir]

# Read the csv files
df_train = pd.read_csv('/content/drive/MyDrive/one-indexed-files-notrash_train.txt', sep=' ', names=['file', 'label'])
df_valid = pd.read_csv('/content/drive/MyDrive/one-indexed-files-notrash_val.txt', sep=' ', names=['file', 'label'])
df_test = pd.read_csv('/content/drive/MyDrive/one-indexed-files-notrash_test.txt', sep=' ', names=['file', 'label'])

"""## Data Pre-Processing"""

# create path column
df_train ['file_path'] = [label_dir[i-1] for i in df_train['label']]+ df_train ['file']
df_valid ['file_path'] = [label_dir[i-1] for i in df_valid['label']]+ df_valid ['file']
df_test ['file_path'] = [label_dir[i-1] for i in df_test['label']]+ df_test ['file']


# create new label column as string type for data augmentation
df_train['label_str'] = df_train['label'].astype(str)
df_valid['label_str'] = df_valid['label'].astype(str)
df_test['label_str'] = df_test['label'].astype(str)

df_train.head()

image_size = 256
labels = ['glass', 'paper', 'cardboard', 'plastic', 'metal', 'trash']

#for the validation and test sets
datagen = ImageDataGenerator(rescale=1./255)

train_set = datagen.flow_from_dataframe(
    df_train,
    x_col='file_path',
    y_col='label_str',
    class_mode='sparse',
    target_size=(image_size, image_size),
    batch_size=32,
    shuffle=True)


valid_set = datagen.flow_from_dataframe(
    df_valid,
    x_col='file_path',
    y_col='label_str',
    class_mode='sparse',
    target_size=(image_size, image_size),
    batch_size=32,
    shuffle=False)

test_set = datagen.flow_from_dataframe(
    df_test,
    x_col='file_path',
    y_col='label_str',
    class_mode='sparse',
    target_size=(image_size, image_size),
    batch_size=32,
    shuffle=False)

#check feature data type
train_set.dtype

train_dir = os.path.join('/content/drive/MyDrive/Garbage-classification/Garbage-classification/')
for label in labels:
    print("Number of " + label + " images:", len(os.listdir(os.path.join(train_dir, label))))

plt.hist(df_train['label'])
plt.xlabel('class')
plt.ylabel('population')

"""## Data Visualization"""

# plotting images of different review for understanding the dataset
train_dir = os.path.join('/content/drive/MyDrive/Garbage-classification/Garbage-classification/')

plt.figure(figsize=(30,14))

for i in range(6):
    directory = os.path.join(train_dir, labels[i])
    for j in range(3):
        path = os.path.join(directory, os.listdir(directory)[j])
        img = mpimg.imread(path)
        
        plt.subplot(6, 10, i*10 + j + 1)
        plt.imshow(img)
        
        if j == 0:
            plt.ylabel(labels[i], fontsize=20)
        
plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);
plt.tight_layout()
plt.savefig("GarbagePicsSample.png")
plt.show()

# checking size of individual image
image = mpimg.imread(os.path.join(os.path.join(glass_dir), os.listdir(os.path.join(glass_dir))[0]))
image.shape

"""# Build a Multi-layer Perceptron

### Build MLP
"""

## Multi Layer Perceptron
def create_MLP(optimizer = 'rmsprop', num_neurons = 100,num_hidden_layers=3, batch = True, activationfunc = 'relu'):
    # Start neural network
    mlp_model = keras.models.Sequential()
    mlp_model.add(keras.layers.Flatten(input_shape=(256, 256, 3)))
    if batch == True:
      mlp_model.add(keras.layers.BatchNormalization())
    for n in range(num_hidden_layers):
      # Add fully connected layer with a ReLU activation function
      mlp_model.add(keras.layers.Dense(num_neurons, activation=activationfunc))
      if batch == True:
        mlp_model.add(keras.layers.BatchNormalization())
    # Add fully connected layer with a sigmoid activation function
    mlp_model.add(keras.layers.Dense(6, activation="softmax"))

    # Compile neural network
    mlp_model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])  

    # Return compiled network
    return mlp_model

"""### Attempt 1 - RMSProp"""

os.getcwd()

os.chdir('/content/drive/MyDrive/COTI_Files')

checkpoint_cb = keras.callbacks.ModelCheckpoint("GT_MLP_model.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)

callbacks_list = [checkpoint_cb, early_stopping_cb]

optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)
MLP = create_MLP(optimizer=optimizer)

history = MLP.fit(train_set, epochs = 20, batch_size = 32, validation_data=(valid_set),callbacks=callbacks_list)

#plot learning curve
import pandas as pd
import matplotlib.pyplot as plt
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()

MLP.summary()

model2 = keras.models.load_model('GT_MLP_model.h5')

score = model2.evaluate(test_set, verbose = 1)
print('Test Accuracy:', score[1])

"""### Attempt 2 - Without BN"""

checkpoint_cb = keras.callbacks.ModelCheckpoint("GT_MLP2_model.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)
callbacks_list = [checkpoint_cb, early_stopping_cb]

MLP2 = create_MLP(optimizer=optimizer,batch = False)

history = MLP2.fit(train_set, epochs = 20, batch_size = 32,  validation_data=(valid_set),callbacks=callbacks_list)

MLP2.summary()

"""### Attempt 3 - Add Learning Rate Schedule"""

# learning rate schedule
def exponential_decay(lr0, s):
	def exponential_decay_fn(epoch):
		return lr0 * 0.1**(epoch / s)
	return exponential_decay_fn

exponential_decay_fn = exponential_decay(lr0=0.01, s=20)
lrate = LearningRateScheduler(exponential_decay_fn )

checkpoint_cb = keras.callbacks.ModelCheckpoint("GT_MLP3_model.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                                  restore_best_weights=True)

callbacks_list = [checkpoint_cb, early_stopping_cb,lrate]

MLP3 = create_MLP()

ML3.summary()

MLP3.fit(train_set, epochs = 20, batch_size = 32, validation_data=(valid_set),callbacks=callbacks_list)

model3 = keras.models.load_model('GT_MLP3_model.h5')

model3.summary()

#plot learning curve
import pandas as pd
import matplotlib.pyplot as plt
pd.DataFrame(model3.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()

score = model3.evaluate(test_set, verbose = 1)
print('Test Accuracy:', score[1])

"""### Attempt 4 - Leaky ReLu"""

## Multi Layer Perceptron
def create_MLP2(optimizer = 'rmsprop', num_neurons = 100,num_hidden_layers=3):
    # Start neural network
    mlp_model = keras.models.Sequential()
    mlp_model.add(keras.layers.Flatten(input_shape=(256, 256, 3)))
    for n in range(num_hidden_layers):
      # Add fully connected layer with a ReLU activation function
      mlp_model.add(keras.layers.Dense(num_neurons, kernel_initializer="he_normal"))
      mlp_model.add(keras.layers.LeakyReLU())
    # Add fully connected layer with a sigmoid activation function
    mlp_model.add(keras.layers.Dense(6, activation="softmax"))

    # Compile neural network
    mlp_model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer, metrics=["accuracy"])  

    # Return compiled network
    return mlp_model

checkpoint_cb = keras.callbacks.ModelCheckpoint("GT_MLP4_model.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)

callbacks_list = [checkpoint_cb, early_stopping_cb]

optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)
MLP4 = create_MLP2(optimizer=optimizer)

history = MLP4.fit(train_set, epochs = 20, batch_size = 32, validation_data=(valid_set),callbacks=callbacks_list)

#plot learning curve
import pandas as pd
import matplotlib.pyplot as plt
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()

score = MLP.evaluate(test_set, verbose = 1)
print('Test Accuracy:', score[1])

model4 = keras.models.load_model('GT_MLP4_model.h5')

model4.summary()

"""# Build a CNN Model

### Attempt 1 - Without Dropout
"""

def create_CNN(optimizer,num_neurons = 100):
    model = Sequential()
    #Add first Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=7, strides=1,padding="SAME", activation="relu",input_shape=(256, 256, 3)))
    #Add max pooling
    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
    #Add second hidden Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,padding="SAME", activation="relu"))
    #Add third hidden Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,padding="SAME", activation="relu"))
    #Add max pooling 2
    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
    #Add flatten layer before dense layer
    model.add(keras.layers.Flatten())
    #Add hidden dense layer
    model.add(keras.layers.Dense(128, activation="relu"))
    #Add dropout
    model.add(keras.layers.Dropout(rate=0.5))
    #Add output layer
    model.add(keras.layers.Dense(6, activation="softmax"))


    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) 
    return model

checkpoint_cb = keras.callbacks.ModelCheckpoint("GT_CNN_model.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                                  restore_best_weights=True)

callbacks_list = [checkpoint_cb, early_stopping_cb]

# Wrap Keras model so it can be used by scikit-learn
CNN = create_CNN(optimizer = 'nadam')

CNN.fit(train_set, epochs = 10, batch_size = 32, validation_data=(valid_set),callbacks=callbacks_list)

CNNmodel1 = keras.models.load_model('GT_CNN_model.h5')

CNNmodel1.summary()

"""### Attempt 2"""

def create_CNN2(optimizer,num_neurons = 100):
    model = Sequential()
    #Add first Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=7, strides=1,padding="SAME", activation="relu",input_shape=(256, 256, 3)))
    #Add max pooling
    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
    #Add second hidden Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,padding="SAME", activation="relu"))
    #Add max pooling 2
    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
    #Add second hidden Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,padding="SAME", activation="relu"))
    #Add max pooling 2
    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
    #Add flatten layer before dense layer
    model.add(keras.layers.Flatten())
    #Add hidden dense layer
    model.add(keras.layers.Dense(128, activation="relu"))
    #Add output layer
    model.add(keras.layers.Dense(6, activation="softmax"))


    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) 
    return model

checkpoint_cb = keras.callbacks.ModelCheckpoint("GT_CNN2_model.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                                  restore_best_weights=True)

callbacks_list = [checkpoint_cb, early_stopping_cb]

# Wrap Keras model so it can be used by scikit-learn
CNN2 = create_CNN2(optimizer = 'nadam')

CNN2.fit(train_set, epochs = 10, batch_size = 64, validation_data=(valid_set),callbacks=callbacks_list)

CNNmodel2 = keras.models.load_model('GT_CNN2_model.h5')

CNNmodel2.summary()

"""### Attempt 3"""

def create_CNN(optimizer,num_neurons = 100):
    model = Sequential()
    #Add first Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=7, strides=1,padding="SAME", activation="relu",input_shape=(256, 256, 3)))
    model.add(keras.layers.BatchNormalization())
    #Add second hidden Conv2D layer
    model.add(keras.layers.Conv2D(filters=64, kernel_size=3, strides=1,padding="SAME", activation="relu"))
    model.add(keras.layers.BatchNormalization())
    #Add max pooling 2
    model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))
    model.add(keras.layers.BatchNormalization())
    #Add dropout
    model.add(keras.layers.Dropout(rate=0.25))
    #Add flatten layer before dense layer
    model.add(keras.layers.Flatten())
    #Add hidden dense layer
    model.add(keras.layers.Dense(128, activation="relu"))
    model.add(keras.layers.BatchNormalization())
    #Add dropout
    model.add(keras.layers.Dropout(rate=0.5))
    #Add output layer
    model.add(keras.layers.Dense(6, activation="softmax"))


    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) 
    return model

# learning rate schedule
def step_decay(epoch):
	initial_lrate = 0.1
	drop = 0.5
	epochs_drop = 10.0
	lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
	return lrate

lrate = LearningRateScheduler(step_decay)

checkpoint_cb = keras.callbacks.ModelCheckpoint("GT_CNN_model.h5", save_best_only=True)
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                                  restore_best_weights=True)

callbacks_list = [checkpoint_cb, early_stopping_cb,lrate]

# Wrap Keras model so it can be used by scikit-learn
CNN = create_CNN(optimizer = 'rmsprop')

CNN.summary()

CNN.fit(train_set, epochs = 10, batch_size = 128, validation_data=(valid_set),callbacks=callbacks_list)



score = CNN.evaluate(test_set, verbose = 1)
print('Test Accuracy:', score[1])

"""## Pre-trained model - ResNet"""

from tensorflow.keras.applications.resnet50 import preprocess_input

datagen_aug = ImageDataGenerator(
    horizontal_flip=True,
    vertical_flip=True,
    shear_range=0.1,
    zoom_range=0.2,
    preprocessing_function=preprocess_input,
    rotation_range=90)

train_set = datagen_aug.flow_from_dataframe(
    df_train,
    x_col='file_path',
    y_col='label_str',
    class_mode='sparse',
    target_size=(image_size, image_size),
    batch_size=32,
    shuffle=True)

#for the validation and test sets
datagen = ImageDataGenerator(preprocessing_function=preprocess_input)


valid_set = datagen.flow_from_dataframe(
    df_valid,
    x_col='file_path',
    y_col='label_str',
    class_mode='sparse',
    target_size=(image_size, image_size),
    batch_size=32,
    shuffle=False)

test_set = datagen.flow_from_dataframe(
    df_test,
    x_col='file_path',
    y_col='label_str',
    class_mode='sparse',
    target_size=(image_size, image_size),
    batch_size=32,
    shuffle=False)

#instantiate a base model with pre-trained weights
KerasModel=tf.keras.applications.resnet50.ResNet50(include_top=False, input_shape=(256,256,3),classes=6, weights='imagenet')

#freeze the base model
for layer in KerasModel.layers:
    layer.trainable = False

new_model = keras.models.Sequential()
new_model.add(KerasModel)
new_model.add(keras.layers.Flatten())
new_model.add(keras.layers.Dense(6, activation="softmax"))

new_model.summary()

InitSeed = 767
tf.random.set_seed(InitSeed)
np.random.seed( InitSeed)

new_model.compile(loss="sparse_categorical_crossentropy",  optimizer="adam", metrics=["accuracy"])

new_model.fit(train_set, epochs = 10, batch_size = 32, validation_data=(valid_set))

score = new_model.evaluate(test_set, verbose = 1)
print('Test Accuracy:', score[1])

